{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Modellalternative 1:\n",
    "\n",
    "Verwendung einer Matrixstruktur basiertem Attentionmechanism um in effizienter Zeit hirachische Strukturen im bereich NLP bzw. Computer Vision zu erfassen. Anders als in der VL vorgestellte Transformer, ermöglicht es somit besser, abhängigkeiten innerhalb der einzelnen Tokens darzustellen.\n",
    "\n",
    "Quelle: \"H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences by Zhenhai Zhu and Radu Soricut\" : https://aclanthology.org/2021.acl-long.294.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modellalternative 2:\n",
    "\n",
    "Verringerung der Attention im Attentionmechanismus durch alternative von ReLU Function um natürlichere erwartete Attention abzubilden. So lässt sich durch Rectified Linear Attention den Attentionmechanismus auf Ebenen der Wörter bzw. Teilwörter ausdehnen, und so wahrscheinlich bessere Leistung in Domainspezifischen Aufgaben erziehlen.\n",
    "\n",
    "Quelle: \"Sparse Attention with Linear Units by Biao Zhang, Ivan Titov, and Rico Sennrich\" : https://aclanthology.org/2021.emnlp-main.523.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
