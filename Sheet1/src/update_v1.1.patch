diff --git a/nlpds/abc/ex1.py b/nlpds/abc/ex1.py
index 1720723..2d04c00 100644
--- a/nlpds/abc/ex1.py
+++ b/nlpds/abc/ex1.py
@@ -19,13 +19,21 @@ class ContextWindowABC(abc.ABC):
     context: List[int]
 
     def __len__(self) -> int:
-        """Get the actual lenght of the context."""
+        """Get the actual length of the context.
+
+        Returns:
+            int: The total number of tokens in the context window to the left and right.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}.__len__() is not implemented"
         )
 
     def as_tuple(self) -> Tuple[int, List[int]]:
-        """Get the target and context as a tuple."""
+        """Get the target and context as a tuple.
+
+        Returns:
+            Tuple[int, List[int]]: A tuple of the target index and a single list containing the left and right context window's token ids, in order.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}.as_tuple() is not implemented"
         )
@@ -35,8 +43,10 @@ class TokenizedSentenceABC:
     input_ids: List[int]
 
     def __init__(self, input_ids: List[int]):
-        """
-        Construct a new tokenized sentence object from the given input ids.
+        """Construct a new tokenized sentence object from the given input ids.
+
+        Args:
+            input_ids (List[int]): The input token ids.
         """
         raise NotImplementedError(
             f"{self.__class__.__name__}.from_input_ids() is not implemented"
@@ -45,9 +55,15 @@ class TokenizedSentenceABC:
     def get_windows(
         self, window_size: int, subsampling_probabilities: np.ndarray
     ) -> List[ContextWindowABC]:
-        """
-        Generate all context windows for this sentence.
+        """Generate all context windows for this sentence.
         Apply the subsampling rules outlined in the paper according to the given subsampling probabilities.
+
+        Args:
+            window_size (int): The window size to the left and right of each target word.
+            subsampling_probabilities (np.ndarray): The subsampling probabilities for all tokens in the vocabulary. The values denote the probability to *keep* each token.
+
+        Returns:
+            List[ContextWindowABC]: The resulting list of context windows
         """
         raise NotImplementedError(
             f"{self.__class__.__name__}.get_windows() is not implemented"
@@ -58,31 +74,64 @@ class TokenizerABC(abc.ABC):
     vocabulary: Dict[str, int]
 
     def __init__(self, lowercase: bool = False):
-        """Tokenizer constructor. If lowercase=True is given, all casing is ignored during training AND on later use."""
+        """Tokenizer constructor.
+
+        Args:
+            lowercase (bool, optional): If True, all casing is ignored during training AND on later use. Defaults to False.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}.__init__() is not implemented"
         )
 
     def train(self, dataset: List[str], max_vocab_size: int):
-        """Train a tokenizer on the given dataset. The final vocabulary size may not exceed the given maximum."""
+        """Train a tokenizer on the given dataset.
+        The final vocabulary size may not exceed the given maximum size.
+
+        Args:
+            dataset (List[str]): The dataset to use for training the tokenizer.
+            max_vocab_size (int): The maximum vocabulary size.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}.train() is not implemented"
         )
 
     def _tokens_to_input_ids(self, tokens: List[str]) -> List[int]:
-        """Convert a list of token strings to their corresponding input ids."""
+        """Convert a list of token strings to their corresponding input ids.
+        All tokens not present in the vocabulary will be removed.
+
+        Args:
+            tokens (List[str]): The tokens to convert to their respective ids.
+
+        Returns:
+            List[int]: A list of token ids.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}._tokens_to_input_ids() is not implemented"
         )
 
     def _pre_tokenize(self, sentence: str) -> List[str]:
-        """Perform pre-tokenization on the given sentence."""
+        """Perform pre-tokenization on the given sentence.
+
+        Args:
+            sentence (str): The sentence to pre-tokenize.
+
+        Returns:
+            List[str]: The list of token strings.
+        """
+
         raise NotImplementedError(
             f"{self.__class__.__name__}._pre_tokenize() is not implemented"
         )
 
     def tokenize(self, sentence: str) -> TokenizedSentenceABC:
-        """Tokenize the given sentence."""
+        """Tokenize the given sentence.
+
+        Args:
+            sentence (str): The sentence to tokenize.
+
+        Returns:
+            TokenizedSentenceABC: The tokenized sentence.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}.tokenize() is not implemented"
         )
@@ -95,39 +144,85 @@ class DatasetABC(abc.ABC):
 
     @classmethod
     def with_tokenizer(cls, corpus: List[str], tokenizer: TokenizerABC):
-        """
-        Create a new dataset object from the given raw corpus using the given tokenizer.
+        """Create a new dataset object from the given raw corpus using the given tokenizer.
 
         Hint: Count the number of occurrences for each token, you will need it!
+
+        Args:
+            corpus (List[str]): The raw input corpus.
+            tokenizer (TokenizerABC): The tokenizer to use.
         """
         raise NotImplementedError(f"{cls.__name__}.with_tokenizer() is not implemented")
 
     def __len__(self) -> int:
-        """Get the size of this dataset, i.e. the number of sentences."""
+        """Get the size of this dataset, i.e. the number of sentences.
+
+        Returns:
+            int: The size of this dataset.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}.__len__() is not implemented"
         )
 
     def dataloader(
-        self, window_size: int, shuffle: bool = False
+        self, window_size: int, threshold: float = 0.001, shuffle: bool = False
     ) -> Iterator[List[ContextWindowABC]]:
         """
         Create a dataloader from this dataset, i.e. an Iterator that returns a list of all windows according to given window size for each sentence.
         If shuffle=True is given, the order of sentences should be randomized.
 
-        For background on the requirements for an Iterator, see: https://docs.python.org/3/library/collections.abc.html#collections-abstract-base-classes
+        Hint:
+            For background on the requirements for an Iterator, see: https://docs.python.org/3/library/collections.abc.html#collections-abstract-base-classes
+
+        Args:
+            window_size (int): The window size to the left and right of the target word.
+            threshold (float, optional): The subsampling threshold. Defaults to 0.001.
+            shuffle (bool, optional): If True, the order of sentences should be randomized. Defaults to False.
+
+        Yields:
+            Iterator[List[ContextWindowABC]]: An iterator of lists containing all ContextWindows for each sentence in the dataset.
         """
         raise NotImplementedError(
             f"{self.__class__.__name__}.iter() is not implemented"
         )
 
-    def get_word_frequencies(self) -> np.ndarray:
-        """
-        Get the raw counts of each token from the given input corpus.
+    @property
+    def token_counts(self) -> np.ndarray:
+        """Get the raw counts of each token from the given input corpus.
         Should return an array of counts, where each entry (row) corresponds to the token id.
+
+        Returns:
+            np.ndarray: A numpy array of integers containing the number of occurrences for each token from the vocabulary of the used tokenizer, according to the tokens' occurrence in the corpus.
+        """
+        raise NotImplementedError(
+            f"{self.__class__.__name__}.word_counts property getter is not implemented"
+        )
+
+    @property
+    def token_frequencies(self) -> np.ndarray:
+        """Get the frequency of each token in the given input corpus.
+        Should return an array of floats, where each entry (row) corresponds to the token id.
+
+        Returns:
+            np.ndarray: A numpy array of floats containing the number of occurrences for each token from the vocabulary of the used tokenizer, according to the tokens' occurrence in the corpus.
+        """
+        raise NotImplementedError(
+            f"{self.__class__.__name__}.word_frequencies property getter is not implemented"
+        )
+
+    def subsampling_probability(self, threshold: float = 0.001) -> np.ndarray:
+        """Get the probabilities for *keeping* a token.
+        Should return an array of floats, where each entry (row) corresponds to the token id.
+        All values should be between 0.0 and 1.0.
+
+        Args:
+            threshold (float, optional): The subsampling threshold. Defaults to 0.001.
+
+        Returns:
+            np.ndarray: A numpy array of floats containing the probability to keep each token from the vocabulary of the used tokenizer, according to the tokens' occurrence count in the corpus.
         """
         raise NotImplementedError(
-            f"{self.__class__.__name__}.get_word_frequencies() is not implemented"
+            f"{self.__class__.__name__}.subsampling_probability() is not implemented"
         )
 
 
@@ -136,70 +231,101 @@ class DatasetABC(abc.ABC):
 ##########################
 
 
-def onehot_vector(indices: List[int], size: int) -> Tensor:
-    """Returns a 2D one-hot encoded Tensor of the given size for the given indices."""
-    raise NotImplementedError(f"onehot_vector() is not implemented")
-
-
 class Word2VecABC(torch.nn.Module, abc.ABC):
     vocab_size: int
     embedding_dim: int
+    # Input or target embedding tensor. Must have require_grad=True.
     embedding_input: Tensor
+    # Output or context embedding tensor. Must have require_grad=True.
     embedding_output: Tensor
-    subsampling_threshold: float
 
     def __init__(
         self,
         vocab_size: int,
         embedding_dim: int,
-        subsampling_threshold: float = 10e-5,
     ):
-        """The word2vec model base class."""
+        """The word2vec model base class.
+
+        Args:
+            vocab_size (int): The total size of the vocabulary.
+            embedding_dim (int): The size of the input and output embeddings.
+        """
         raise NotImplementedError(
             f"{self.__class__.__name__}.__init__() is not implemented"
         )
 
-    def _init_weights(self):
+    def init_weights(self):
         """
         (Re-)Initialize the weights (embeddings).
         Should be called from within Wor2VecSkipGramABC.__init__()
         """
         raise NotImplementedError(
-            f"{self.__class__.__name__}._init_weights() is not implemented"
+            f"{self.__class__.__name__}.init_weights() is not implemented"
         )
 
-    def forward(self, target: int, context: List[int]) -> Tuple[Tensor, Tensor]:
-        """
-        Perform a forward pass, i.e. get the input embeddings (1D Tensor, size: 1 x e) and
-        the context embeddings (2D Tensor, size: b x e), where e is the embedding dimensionality.
+    def set_weights(self, weights_input: Tensor, weights_output: Tensor):
+        """Set the weights to the given values.
+
+        Args:
+            weights_input (Tensor): Input or target embedding values.
+            weights_output (Tensor): Output or context embedding values.
         """
         raise NotImplementedError(
-            f"{self.__class__.__name__}.forward() is not implemented"
+            f"{self.__class__.__name__}.set_weights() is not implemented"
         )
 
-    def _calculate_loss(
-        self, target_embedding: Tensor, context_embeddings: Tensor
-    ) -> Tensor:
-        """
-        Calculate the total loss for the skip-gram objective for the given target and its context's embeddings.
+    def embed_input(self, *token_ids: int) -> Tensor:
+        """Get the input embeddings of the given tokens.
+
+        Args:
+            token_ids (*int): Any number of token ids.
+
+        Returns:
+            Tensor: A single Tensor of shape (len(token_ids), self.embedding_dim).
         """
         raise NotImplementedError(
-            f"{self.__class__.__name__}._calculate_loss() is not implemented"
+            f"{self.__class__.__name__}.embed_input() is not implemented"
         )
 
-    def train(self, dataset: DatasetABC, epochs: int, learning_rate: float):
-        """
-        Run word2vec training on the given dataset for the given number of epochs (called 'iterations' in the paper) with the given learning rate.
+    def embed_output(self, *token_ids: int) -> Tensor:
+        """Get the output embeddings of the given tokens.
+
+        Args:
+            token_ids (*int): Any number of token ids.
+
+        Returns:
+            Tensor: A single Tensor of shape (len(token_ids), self.embedding_dim).
         """
         raise NotImplementedError(
-            f"{self.__class__.__name__}.train() is not implemented"
+            f"{self.__class__.__name__}.embed_output() is not implemented"
         )
 
-    def _train_epoch(self, dataset: DatasetABC, optim: torch.optim.Adagrad):
+    def forward(self, target: int | Tensor, context: List[int] | Tensor) -> Tensor:
+        """Perform a forward pass, i.e. calculate the dot-product of the embedding of the given target token with each embedding of the given context tokens
+        Args:
+            target (int | Tensor): The target token id or a 1D embedding tensor.
+            context (List[int] | Tensor): Any number of context token ids or their embeddings in a 2D Tensor.
+
+        Hint:
+            Either pass the raw token ids to this method or embed the tokens first, e.g. using the appropriate embed_*() method.
+
+        Returns:
+            Tensor: Returns a 1D Tensor of size c, where c is the number of given context tokens, containing the dot-products of the target embedding with the context embeddings.
         """
-        Run word2vec training for a single epoch.
-        Should be called from within Wor2VecSkipGramABC.train().
+        raise NotImplementedError(
+            f"{self.__class__.__name__}.forward() is not implemented"
+        )
+
+    def calculate_objective(self, *args, **kwargs) -> Tensor:
+        """Calculate the objective for the given sample.
+
+        Note:
+            You do not need to use this method in your training loop, if there is a more efficient way to obtain the loss.
+            You must, however, implement this method for evaluation purposes.
+
+        Returns:
+            Tensor: The SoftMax loss of the given sample.
         """
         raise NotImplementedError(
-            f"{self.__class__.__name__}._train_epoch() is not implemented"
+            f"{self.__class__.__name__}.calculate_objective() is not implemented"
         )
diff --git a/nlpds/submission/ex1.py b/nlpds/submission/ex1.py
index 30b375c..94036b1 100644
--- a/nlpds/submission/ex1.py
+++ b/nlpds/submission/ex1.py
@@ -18,17 +18,27 @@ from nlpds.abc.ex1 import (
 
 @dataclass
 class ContextWindow(ContextWindowABC):
-    """Example: this is how you may implement the interfaces."""
+    """Example: This is how you may implement the interfaces."""
 
+    # Target input id
     target: int
+    # Context input ids
     context: List[int]
 
     def __len__(self) -> int:
-        """Get the lenght of the context."""
+        """Get the actual length of the context.
+
+        Returns:
+            int: The total number of tokens in the context window to the left and right.
+        """
         return len(self.context)
 
     def as_tuple(self) -> Tuple[int, List[int]]:
-        """Get the target and context as a tuple."""
+        """Get the target and context as a tuple.
+
+        Returns:
+            Tuple[int, List[int]]: A tuple of the target index and a single list containing the left and right context window's token ids, in order.
+        """
         return self.target, self.context
 
 
@@ -49,16 +59,110 @@ class Dataset(DatasetABC):
 ##########################
 
 
-def onehot_vector(indices: List[int], size: int) -> Tensor:
-    raise NotImplementedError(f"onehot_vector() is not implemented")
+def onehot_vector(indices: List[int], size: int) -> Tensor:  # TODO
+    """Returns a 2D one-hot encoded Tensor of the given size for the given indices.
 
+    Args:
+        indices (List[int]): A list of token indices.
+        size (int): The size of the one-hot vectors.
 
-class SkipGramSoftMax(Word2VecABC):
-    pass  # TODO
+    Returns:
+        Tensor: A 2D tensor of shape (len(indices), size).
+    """
+    raise NotImplementedError(f"onehot_vector() is not implemented")
 
 
-class SkipGramNegativeSampling(Word2VecABC):
-    pass  # TODO
+# Hint: It may be useful to introduce another class like 'Word2VecBase'
+# that inherits from Word2VecABC instead to remain true to the DRY principle.
+#   - DRY => Don't Repeat Yourself
+#
+# While not required, it could look like this:
+#
+# class Word2VecBase(Word2VecABC):
+#     pass
+# class SkipGramSoftMax(Word2VecBase):
+#     pass
+# class SkipGramNegativeSampling(Word2VecBase):
+#     pass
+
+
+class SkipGramSoftMax(Word2VecABC):  # TODO
+    def calculate_objective(self, target_id: int, context_id: int) -> Tensor:
+        """Calculate the SoftMax objective according to equations (1) & (2) in the word2vec paper (Mikolov, 2013, NIPS).
+
+        Note:
+            You do not need to use this method in your training loop.
+            You must, however, implement this method for evaluation purposes.
+
+        Returns:
+            Tensor: The SoftMax objective for the given sample.
+        """
+        raise NotImplementedError(
+            f"{self.__class__.__name__}.calculate_loss() is not implemented"
+        )
+
+
+class SkipGramNegativeSampling(Word2VecABC):  # TODO
+    def calculate_objective(
+        self, target_id: int, context_id: int, negative_sample_ids: List[int]
+    ) -> Tensor:
+        """Calculate the Negative Sampling objective according to equation (4) in the word2vec paper (Mikolov, 2013, NIPS).
+
+        Note:
+            You do not need to use this method in your training loop.
+            You must, however, implement this method for evaluation purposes.
+
+        Returns:
+            Tensor: The Negative Sampling objective for the given sample.
+        """
+        raise NotImplementedError(
+            f"{self.__class__.__name__}.calculate_loss() is not implemented"
+        )
+
+
+##################
+# Training Loops #
+##################
+
+
+def train_sf(
+    model: SkipGramSoftMax,
+    dataset: Dataset,
+    epochs: int,
+    learning_rate: float,
+    threshold: float,
+):
+    """Run word2vec training using SoftMax on the given dataset for the given number of epochs (called 'iterations' in the paper) with the given learning rate.
+
+    Args:
+        model (SkipGramSoftMax): The word2vec model.
+        dataset (Dataset): The dataset to use for training.
+        epochs (int): The number of training iterations.
+        threshold (float): The sub-sampling threshold to use.
+        learning_rate (float): The learning rate to use.
+    """
+    raise NotImplementedError(f"train_sf() is not implemented")
+
+
+def train_ns(
+    model: SkipGramNegativeSampling,
+    dataset: Dataset,
+    epochs: int,
+    learning_rate: float,
+    threshold: float,
+    k: int,
+):
+    """Run word2vec training using Negative Sampling with k given negative samples on the given dataset for the given number of epochs (called 'iterations' in the paper) with the given learning rate.
+
+    Args:
+        model (SkipGramNegativeSampling): The word2vec model.
+        dataset (Dataset): The dataset to use for training.
+        epochs (int): The number of training iterations.
+        learning_rate (float): The learning rate to use.
+        threshold (float): The sub-sampling threshold to use.
+        k (float): The number of negative samples to use.
+    """
+    raise NotImplementedError(f"train_ns() is not implemented")
 
 
 ###############
diff --git a/test_ex1.py b/test_ex1.py
index 667f331..26dfff4 100644
--- a/test_ex1.py
+++ b/test_ex1.py
@@ -23,11 +23,19 @@ def test_window():
         [0, 1, 2, 3, 4],
     )
     windows = tokens.get_windows(2, np.ones(5))
-    assert windows[0].target == 0
-    assert windows[0].context == [2, 3]
+    assert windows[0].target == 0, f"expected: {0}, got: {windows[0].target}"
+    assert windows[0].context == [
+        1,
+        2,
+    ], f"expected: {[1, 2]}, got: {windows[0].context}"
 
-    assert windows[2].target == 0
-    assert windows[0].context == [0, 1, 3, 4]
+    assert windows[2].target == 2, f"expected: {0}, got: {windows[2].target}"
+    assert windows[2].context == [
+        0,
+        1,
+        3,
+        4,
+    ], f"expected: {[0, 1, 3, 4]}, got: {windows[2].context}"
 
 
 def test_tokenizer():
@@ -55,10 +63,61 @@ def test_tokenizer():
         actual_idx = tokenizer.vocabulary.get(word)
         assert (
             actual_idx == expected_idx
-        ), f"Expected {word} to have index {expected_idx} but got {actual_idx}"
+        ), f"Expected '{word}' to have index {expected_idx} but got {actual_idx}"
 
+    return tokenizer
 
-VERBOSE: Final[int] = 0
+
+def test_dataset():
+    from nlpds.submission.ex1 import Dataset, Tokenizer
+
+    tokenizer = test_tokenizer()
+
+    dataset = Dataset.with_tokenizer(tiny_corpus, tokenizer)
+
+    assert len(dataset) == len(
+        tiny_corpus
+    ), f"expected dataset length to be equal to tiny_corpus length but got: {len(dataset)} != {len(tiny_corpus)}"
+
+    actual = list(dataset.token_counts)[:5]
+    expected = [19, 17, 14, 13, 11]
+    assert actual == expected, f"expected: {expected}, got: {actual}"
+
+    assert len(dataset.token_counts) == len(
+        dataset.token_frequencies
+    ), "word counts and word frequencies should be of equal length"
+
+
+def test_word2vec():
+    import torch
+    from nlpds.submission.ex1 import SkipGramSoftMax
+
+    vocab_size = 25
+    emb_dim = 10
+
+    model = SkipGramSoftMax(vocab_size, emb_dim)
+
+    emb_target = model.embed_input(2)
+    assert emb_target.shape == (1, emb_dim)
+
+    emb_context = model.embed_output(0, 1, 3, 4)
+    assert emb_context.shape == (4, emb_dim)
+
+    output_ids = model.forward(2, [0, 1, 3, 4])
+    output_tensors = model.forward(emb_target, emb_context)
+
+    assert torch.equal(
+        output_ids, output_tensors
+    ), "Expected the forward result to be equal for both id and tensor inputs"
+
+    # By setting all weights to 1 ...
+    model.set_weights(torch.ones((25, 10)), 2 * torch.ones((25, 10)))
+
+    # ... this should be equal to -3.2189
+    assert abs(model.calculate_objective(0, 1).item() + 3.2189) < 0.0001
+
+
+VERBOSE: Final[int] = 1
 
 if __name__ == "__main__":
     try:
@@ -77,4 +136,20 @@ if __name__ == "__main__":
         if VERBOSE:
             traceback.print_exc()
 
-    # Note: There will be an update of the exercise sheets with more tests!
+    try:
+        test_dataset()
+        print("test_dataset: passed")
+    except:
+        print("test_dataset: failed")
+        if VERBOSE:
+            traceback.print_exc()
+
+    try:
+        test_word2vec()
+        print("test_word2vec: passed")
+    except:
+        print("test_word2vec: failed")
+        if VERBOSE:
+            traceback.print_exc()
+
+    # Note: There will be further updates of the exercise sheets with more tests!
